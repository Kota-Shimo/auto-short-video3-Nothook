#!/usr/bin/env python
"""
main.py â€“ VOCABå°‚ç”¨ç‰ˆï¼ˆå˜ç´”çµåˆï¼‹æ—¥æœ¬èªãµã‚ŠãŒãª[TTSã®ã¿]ï¼‹å…ˆé ­ç„¡éŸ³ï¼‹æœ€çŸ­1ç§’ï¼‰
- ä¾‹æ–‡ã¯å¸¸ã«ã€Œ1æ–‡ã ã‘ã€ã€‚ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å¤±æ•—æ™‚ã¯æœ€å¤§5å›ï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰ã¯æœ€å¤§7å›ï¼‰ã¾ã§å†ç”Ÿæˆã—ã€æœ€å¾Œã¯ãƒ•ã‚§ãƒ¼ãƒ«ã‚»ãƒ¼ãƒ•ï¼ˆå¤šè¨€èªï¼‰ã€‚
- ç¿»è¨³ï¼ˆå­—å¹•ï¼‰ã¯1è¡ŒåŒ–ã—ã€è¤‡æ–‡ã¯å…ˆé ­1æ–‡ã®ã¿æ¡ç”¨ã€‚URL/çµµæ–‡å­—/ä½™åˆ†ãªç©ºç™½ã‚’é™¤å»ã€‚
- è¿½åŠ : TARGET_ACCOUNT/--account ã§ combos ã‚’ã‚¢ã‚«ã‚¦ãƒ³ãƒˆå˜ä½ã«çµã‚Šè¾¼ã¿å¯èƒ½ã€‚
- è¿½åŠ : topic_picker ã®æ–‡è„ˆãƒ’ãƒ³ãƒˆï¼ˆcontextï¼‰ã‚’ä¾‹æ–‡ç”Ÿæˆã«æ¸¡ã—ã¦æ—¥æœ¬èªå´©ã‚Œã‚’æŠ‘åˆ¶ã€‚
- è¿½åŠ : ãƒ©ãƒ³ã‚°ã‚¨ãƒ¼ã‚¸ãƒ«ãƒ¼ãƒ«ï¼ˆå³å¯†ãƒ¢ãƒãƒªãƒ³ã‚¬ãƒ«ãƒ»è¨˜å·/æ³¨é‡ˆç¦æ­¢ï¼‰ã‚’ä¾‹æ–‡ç”Ÿæˆã«çµ±åˆã€‚
- è¿½åŠ : å˜èª2è¡Œã®å­—å¹•ã¯ã€Œä¾‹æ–‡ï¼‹ãƒ†ãƒ¼ãƒï¼‹å“è©ãƒ’ãƒ³ãƒˆã€ã§1èªã«ç¢ºå®šã™ã‚‹æ–‡è„ˆè¨³ã¸åˆ‡æ›¿ã€‚
- è¿½åŠ : æ—¥æœ¬èªã®ä¾‹æ–‡è¡Œã‚‚â€œã‹ãªèª­ã¿â€ã«å¤‰æ›ã—ã¦èª­ã¿ä¸Šã’å¯èƒ½ï¼ˆå­—å¹•ã¯åŸæ–‡ã®ã¾ã¾ï¼‰ã€‚
- è¿½åŠ : ğŸ‡¯ğŸ‡µæ—¥æœ¬èª/ğŸ‡°ğŸ‡·éŸ“å›½èªã«ãƒ­ãƒ¼ãƒå­—å­—å¹•ï¼ˆja-Latn / ko-Latnï¼‰ã‚’ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¿½åŠ ï¼ˆSUB_ROMAJI_JA / SUB_ROMAN_KOï¼‰
- è¿½åŠ : ğŸ‡¯ğŸ‡µæ—¥æœ¬èªã®ãµã‚ŠãŒãªå­—å¹•ï¼ˆja-kanaï¼‰ã‚’ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¿½åŠ ï¼ˆSUB_KANA_JAï¼‰
"""

import argparse, logging, re, json, subprocess, os, sys
from datetime import datetime
from pathlib import Path
from shutil import rmtree

import yaml
from pydub import AudioSegment
from openai import OpenAI

from config         import BASE, OUTPUT, TEMP
from translate      import translate
from audio_fx       import enhance
from bg_image       import fetch as fetch_bg
from thumbnail      import make_thumbnail
from upload_youtube import upload
from topic_picker   import pick_by_content_type
# â˜… ãƒˆãƒ¬ãƒ³ãƒ‰å–å¾—ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆGoogle News RSS ç­‰ï¼‰
from trend_fetcher  import get_trend_candidates
import datetime as dt   # UTCæ—¥ä»˜ã§å®‰å®šé¸æŠã«ä½¿ã†


# æ—§:
# from tts_openai     import speak

# æ–°: ç½®ãæ›ãˆ
from tts_openai import speak as speak_openai
try:
    from tts_google import speak as speak_google
except Exception:
    speak_google = None

def speak(lang_code, speaker, text, out_path, style='neutral'):
    """
    æ—¥æœ¬èª + TTS_ENGINE_JA=google + tts_google ãŒä½¿ãˆã‚‹ â†’ Google TTS
    ãã‚Œä»¥å¤– â†’ å¾“æ¥ã® OpenAI TTS
    """
    use_google = (
        (lang_code or "").lower() == "ja" and
        os.getenv("TTS_ENGINE_JA", "").strip().lower() == "google" and
        speak_google is not None
    )
    if use_google:
        logging.info("[TTS] Google TTS (ja) ã‚’ä½¿ç”¨")
        return speak_google("ja", speaker, text, out_path, style)
    logging.info(f"[TTS] OpenAI TTS ã‚’ä½¿ç”¨ lang={lang_code}")
    return speak_openai(lang_code, speaker, text, out_path, style)
    
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GPT = OpenAI()
CONTENT_MODE = "vocab"
DEBUG_SCRIPT = os.getenv("DEBUG_SCRIPT", "0") == "1"
GAP_MS       = int(os.getenv("GAP_MS", "120"))
PRE_SIL_MS   = int(os.getenv("PRE_SIL_MS", "120"))
MIN_UTTER_MS = int(os.getenv("MIN_UTTER_MS", "1000"))

# â˜… æ—¥æœ¬èªã ã‘å€‹åˆ¥ã«èª¿æ•´ã§ãã‚‹ENVï¼ˆæœªè¨­å®šãªã‚‰é€šå¸¸å€¤ã‚’ç¶™æ‰¿ï¼‰
GAP_MS_JA       = int(os.getenv("GAP_MS_JA", str(GAP_MS)))
PRE_SIL_MS_JA   = int(os.getenv("PRE_SIL_MS_JA", str(PRE_SIL_MS)))
MIN_UTTER_MS_JA = int(os.getenv("MIN_UTTER_MS_JA", "800"))  # ãƒ‡ãƒ•ã‚©è»½ã‚çŸ­ç¸®

# â˜… ä¾‹æ–‡ã®â€œã‹ãªèª­ã¿â€ãƒˆã‚°ãƒ«
#   off  : ã‹ãªèª­ã¿ã—ãªã„ï¼ˆåŸæ–‡ã®ã¾ã¾ï¼‰
#   on   : å¸¸ã«ã‹ãªèª­ã¿
#   auto : æ–‡é•·ã¨æ¼¢å­—ç‡ã§è‡ªå‹•ï¼ˆæ¨å¥¨ï¼‰
JA_EX_READING = os.getenv("JA_EX_READING", "auto").lower()  # "auto" | "on" | "off"
JA_EX_READING_KANJI_RATIO = float(os.getenv("JA_EX_READING_KANJI_RATIO", "0.25"))
JA_EX_READING_MAX_LEN     = int(os.getenv("JA_EX_READING_MAX_LEN", "60"))

SUB_JA_ONLY_ROMAJI = os.getenv("SUB_JA_ONLY_ROMAJI", "0") == "1"

# ç”Ÿæˆæ™‚ã®æ¸©åº¦ï¼ˆå¿…è¦ãªã‚‰ç’°å¢ƒå¤‰æ•°ã§ä¸Šæ›¸ãï¼‰
EX_TEMP_DEFAULT = float(os.getenv("EX_TEMP", "0.35"))   # ä¾‹æ–‡
LIST_TEMP       = float(os.getenv("LIST_TEMP", "0.30")) # èªå½™ãƒªã‚¹ãƒˆ

LANG_NAME = {
    "en": "English", "pt": "Portuguese", "id": "Indonesian",
    "ja": "Japanese","ko": "Korean", "es": "Spanish", "fr": "French",
}

JP_CONV_LABEL = {
    "en": "è‹±ä¼šè©±", "ja": "æ—¥æœ¬èªä¼šè©±", "es": "ã‚¹ãƒšã‚¤ãƒ³èªä¼šè©±",
    "pt": "ãƒãƒ«ãƒˆã‚¬ãƒ«èªä¼šè©±", "ko": "éŸ“å›½èªä¼šè©±", "id": "ã‚¤ãƒ³ãƒ‰ãƒã‚·ã‚¢èªä¼šè©±",
}

with open(BASE / "combos.yaml", encoding="utf-8") as f:
    COMBOS = yaml.safe_load(f)["combos"]

def reset_temp():
    if TEMP.exists():
        rmtree(TEMP)
    TEMP.mkdir(exist_ok=True)

def sanitize_title(raw: str) -> str:
    title = re.sub(r"^\s*(?:\d+\s*[.)]|[-â€¢ãƒ»])\s*", "", raw)
    title = re.sub(r"[\s\u3000]+", " ", title).strip()
    return title[:97] + "â€¦" if len(title) > 100 else title or "Auto Video"

def _infer_title_lang(audio_lang: str, subs: list[str], combo: dict) -> str:
    if "title_lang" in combo and combo["title_lang"]:
        return combo["title_lang"]
    if len(subs) >= 2:
        return subs[1]
    for s in subs:
        if s != audio_lang:
            return s
    return audio_lang

def resolve_topic(arg_topic: str) -> str:
    return arg_topic

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ãƒ»ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å…±é€š
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_URL_RE   = re.compile(r"https?://\S+")
_NUM_LEAD = re.compile(r"^\s*\d+[\).:\-]\s*")
_QUOTES   = re.compile(r'^[\"â€œâ€\']+|[\"â€œâ€\']+$')
_EMOJI_RE = re.compile(r"[\U00010000-\U0010ffff]")
_SENT_END = re.compile(r"[ã€‚.!?ï¼ï¼Ÿ]")

def _normalize_spaces(t: str) -> str:
    return re.sub(r"\s+", " ", (t or "")).strip()

def _clean_strict(text: str) -> str:
    t = (text or "").strip()
    t = _URL_RE.sub("", t)
    t = _NUM_LEAD.sub("", t)
    t = _QUOTES.sub("", t)
    t = _EMOJI_RE.sub("", t)
    t = re.sub(r"[\:\-â€“â€”]\s*$", "", t)
    return _normalize_spaces(t)

def _is_single_sentence(text: str) -> bool:
    return len(_SENT_END.findall(text or "")) <= 1

def _fits_length(text: str, lang_code: str) -> bool:
    if lang_code in ("ja", "ko", "zh"):
        return len(text or "") <= 34  # â† 30â†’34ã«ç·©å’Œ
    return len(re.findall(r"\b\w+\b", text or "")) <= 13  # â† 12â†’13ã«ç·©å’Œ

def _ensure_period_for_sentence(txt: str, lang_code: str) -> str:
    t = txt or ""
    return t if re.search(r"[ã€‚.!?ï¼ï¼Ÿ]$", t) else t + ("ã€‚" if lang_code == "ja" else ".")

def _clean_sub_line(text: str, lang_code: str) -> str:
    t = _clean_strict(text).replace("\n", " ").strip()
    m = _SENT_END.search(t)
    if m:
        end = m.end()
        t = t[:end]
    return t

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ç¿»è¨³ã®å¼·åŒ–ï¼ˆä¾‹æ–‡ç”¨ï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_ASCII_ONLY = re.compile(r'^[\x00-\x7F]+$')

def _looks_like_english(s: str) -> bool:
    s = (s or "").strip()
    return bool(_ASCII_ONLY.fullmatch(s)) and bool(re.search(r'[A-Za-z]', s))

def _needs_retranslate(output: str, src_lang: str, target_lang: str, original: str) -> bool:
    if target_lang == src_lang:
        return False
    out = (output or "").strip()
    if not out:
        return True
    if out.lower() == (original or "").strip().lower():
        return True
    if target_lang == "en" and not _looks_like_english(out):
        return True
    return False

def translate_sentence_strict(sentence: str, src_lang: str, target_lang: str) -> str:
    try:
        first = translate(sentence, target_lang)
    except Exception:
        first = ""
    if not _needs_retranslate(first, src_lang, target_lang, sentence):
        return _clean_sub_line(first, target_lang)
    try:
        rsp = GPT.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{
                "role":"user",
                "content":(
                    f"Translate from {LANG_NAME.get(src_lang,'source language')} "
                    f"to {LANG_NAME.get(target_lang,'target language')}.\n"
                    "Return ONLY the translation as a single sentence. "
                    "No explanations, no quotes, no extra symbols.\n\n"
                    f"Text: {sentence}"
                )
            }],
            temperature=0.0, top_p=1.0
        )
        out = (rsp.choices[0].message.content or "").strip()
        out = _clean_sub_line(out, target_lang)
        if out:
            return out
    except Exception:
        pass
    return _clean_sub_line(sentence, target_lang)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ãƒ©ãƒ³ã‚°ã‚¨ãƒ¼ã‚¸ãƒ«ãƒ¼ãƒ«
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _lang_rules(lang_code: str) -> str:
    if lang_code == "ja":
        # ä¸ãˆãŸãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆè‹±å­—/ãƒã‚¤ãƒ•ãƒ³å«ã‚€ï¼‰ã¯ãã®ã¾ã¾è¨±å¯ã™ã‚‹å‰æã«å¤‰æ›´
        return (
            "Write entirely in Japanese. "
            "Do not include other languages. "
            "Avoid ASCII symbols such as '/', 'â†’', '()', '[]', '<>', and '|'. "
            "No translation glosses, brackets, or country/language mentions. "
            "The given token may appear verbatim even if it contains ASCII or a hyphen."
        )
    lang_name = LANG_NAME.get(lang_code, "English")
    return (
        f"Write entirely in {lang_name}. "
        "Do not code-switch or include other writing systems. "
        "Avoid ASCII symbols like '/', 'â†’', '()', '[]', '<>', and '|'. "
        "No translation glosses, brackets, or country/language mentions."
    )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# æ—¥æœ¬èªå‘ã‘ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _guess_ja_pos(word: str) -> str:
    w = (word or "").strip()
    if not w:
        return "noun"
    if w.endswith(("ã™ã‚‹", "ã—ã¾ã™", "ã—ãŸã„", "ã—ãŸ", "ã—ãªã„", "ã—ã‚ˆã†")):
        return "verb"
    if re.search(r"(ã†|ã|ã|ã™|ã¤|ã¬|ã‚€|ã¶|ã‚‹)$", w):
        return "verb"
    if w.endswith("ã„"):
        return "iadj"
    if w.endswith(("çš„", "çš„ãª", "é¢¨")):
        return "naadj"
    if re.fullmatch(r"[ã‚¡-ãƒ¶ãƒ¼]+", w):
        return "noun"
    return "noun"

def _ja_template_fallback(word: str) -> str:
    kind = _guess_ja_pos(word)
    if kind == "verb":
        return f"{word}ã¨ã“ã‚ã§ã™ã€‚"
    if kind == "iadj":
        return f"{word}ã§ã™ã­ã€‚"
    if kind == "naadj":
        return f"{word}ã ã­ã€‚"
    return f"{word}ãŒå¿…è¦ã§ã™ã€‚"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# èªå½™ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _example_temp_for(lang_code: str) -> float:
    return 0.20 if lang_code == "ja" else EX_TEMP_DEFAULT

# --- è¿½åŠ : ãƒã‚¤ãƒ•ãƒ³é¡æ­£è¦åŒ–ãƒ»å«æœ‰åˆ¤å®šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ----------------
_HYPHENS = "â€-â€“â€”âˆ’ï¹˜ï¹£"  # å„ç¨®è¦‹ãŸç›®ãƒã‚¤ãƒ•ãƒ³
def _norm_hyphen(s: str) -> str:
    return re.sub(f"[{_HYPHENS}]", "-", s or "")

def _contains_token_trendaware(surface: str, sent: str, lang_code: str, is_trend: bool) -> bool:
    try:
        s = _norm_hyphen(surface or "")
        t = _norm_hyphen(sent or "")
        if lang_code in ("ja","ko","zh"):
            # ãã®ã¾ã¾ä¸€è‡´
            if s and s in t:
                return True
            # æ—¥æœ¬èªã®ç°¡æ˜“èªå¹¹ä¸€è‡´
            if lang_code == "ja" and len(s) >= 2:
                stem = re.sub(r"[ã¾ã™ã§ã—ãŸã ãŸã„ãªã„ã‚ˆã†ã†ããã™ã¤ã¬ã‚€ã¶ã‚‹ãŸã¦ã§ã‚“]?$", "", s)
                if stem and stem in t:
                    return True
            # ä»£è¡¨ä¾‹: check-in â‡” ãƒã‚§ãƒƒã‚¯ã‚¤ãƒ³ ã‚’åŒä¸€è¦–ï¼ˆã‚ˆãå‡ºã‚‹ãŸã‚ç‰¹åˆ¥æ‰±ã„ï¼‰
            if s.lower() in ("check-in","checkin") and "ãƒã‚§ãƒƒã‚¯ã‚¤ãƒ³" in t:
                return True
            return False
        # éCJKã¯å°æ–‡å­—åŒ…å«ã§ååˆ†
        return s.lower() in t.lower()
    except Exception:
        return True

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ä¾‹æ–‡ç”Ÿæˆï¼ˆå¼·åŒ–ç‰ˆï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _gen_example_sentence(
    word: str,
    lang_code: str,
    context_hint: str = "",
    difficulty: str | None = None,
) -> str:
    lang_name = LANG_NAME.get(lang_code, "English")
    ctx = (context_hint or "").strip()
    diff = (difficulty or "").strip().upper()
    if diff not in ("A1", "A2", "B1", "B2"):
        diff = ""

    # äº’æ›ãƒ•ãƒ©ã‚°ï¼ˆé€šå¸¸å›ã¯ False ã®ã¾ã¾ï¼‰
    is_trend = "[TREND]" in ctx

    rules = _lang_rules(lang_code)

    cefr_guides = {
        "A1": ("Use very simple, high-frequency words. Present tense. One clause only. No subclauses, no passive, no numbers or dates. Keep it short and concrete."),
        "A2": ("Use common everyday words. Prefer present or simple past. One short clause (or two joined by 'and' at most). Avoid complex relative clauses or conditionals."),
        "B1": ("Use clear, everyday language with slightly more detail. One sentence, but may include a short reason or condition. Avoid rare idioms."),
        "B2": ("Use natural, precise language. One sentence with good flow; mild nuance allowed, but avoid overly technical words."),
        "":  (""),  # æœªæŒ‡å®š
    }
    cefr_line = cefr_guides.get(diff, "")

    # åŸºæœ¬ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆé€šå¸¸ï¼‰
    system = {"role":"system","content":"You write exactly ONE natural sentence. No lists, no quotes, no emojis, no URLs. Keep it monolingual."}
    if lang_code == "ja":
        user_normal = (
            f"{rules} å˜èªã€Œ{word}ã€ã‚’**ãã®è¡¨è¨˜ã®ã¾ã¾**å¿…ãšå«ã‚ã¦ã€æ—¥æœ¬èªã§è‡ªç„¶ãªä¸€æ–‡ã‚’ã¡ã‚‡ã†ã©1ã¤ã ã‘æ›¸ã„ã¦ãã ã•ã„ã€‚"
            "ã‹ã£ã“æ›¸ãã‚„ç¿»è¨³æ³¨é‡ˆã¯ä¸è¦ã§ã™ã€‚å¼•ç”¨ç¬¦ã¯ä½¿ã‚ãªã„ã§ãã ã•ã„ã€‚"
        )
        if ctx: user_normal += f" ã‚·ãƒ¼ãƒ³ã®æ–‡è„ˆ: {ctx}"
        if diff:
            user_normal += " é›£æ˜“åº¦ç›®å®‰: " + {
                "A1": "ã”ãç°¡å˜ãªèªã§ã€ç¾åœ¨å½¢ã€‚ç¯€ã¯1ã¤ã ã‘ã€‚",
                "A2": "æ—¥å¸¸èªã§çŸ­ãã€‚ç¾åœ¨å½¢ä¸­å¿ƒã€‚è¤‡é›‘ãªé–¢ä¿‚ç¯€ã¯ä½¿ã‚ãªã„ã€‚",
                "B1": "ã‚ã‹ã‚Šã‚„ã™ãã€‚ç†ç”±ã‚„æ¡ä»¶ã‚’çŸ­ãå«ã‚ã¦ã‚‚ã‚ˆã„ãŒ1æ–‡ã§ã€‚",
                "B2": "è‡ªç„¶ã§æ­£ç¢ºã«ã€‚å°‚é–€çš„ã™ãã‚‹èªã¯é¿ã‘ã‚‹ã€‚"
            }[diff]
    else:
        user_normal = (
            f"{rules} Write exactly ONE short, natural sentence in {lang_name} that uses the token **{word}** verbatim. "
            "Do not add quotes or brackets. Return ONLY the sentence."
        )
        if ctx: user_normal += f" Scene hint: {ctx}"
        if diff: user_normal += f" CEFR level: {diff}. {cefr_line}"

    # ãƒ€ã‚¦ãƒ³ã‚·ãƒ•ãƒˆç”¨ï¼ˆæœ€å¾Œã®2å›ã ã‘ä½¿ã†ï¼‰
    if lang_code == "ja":
        user_simple = (
            f"å˜èªã€Œ{word}ã€ã‚’ãã®è¡¨è¨˜ã®ã¾ã¾å¿…ãšå«ã‚ã¦ã€æ—¥æœ¬èªã§**çŸ­ã„ä¸€æ–‡ã ã‘**ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚"
            "å¼•ç”¨ç¬¦ã‚„æ‹¬å¼§ã€ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã¯ä½¿ã‚ãªã„ã§ãã ã•ã„ã€‚æœ€å¾Œã¯å¥ç‚¹ã€‚"
        )
    else:
        user_simple = (
            f"Write ONE very short {lang_name} sentence that includes **{word}** verbatim. "
            "No quotes or brackets. End with a period."
        )

    # æ¸©åº¦
    if   diff == "A1": base_temp = 0.10
    elif diff == "A2": base_temp = 0.18 if lang_code == "ja" else 0.22
    elif diff == "B1": base_temp = 0.28
    elif diff == "B2": base_temp = 0.32
    else:              base_temp = _example_temp_for(lang_code)
    local_temp = max(0.05, base_temp - 0.05) if is_trend else base_temp

    # é•·ã•åˆ¶ç´„ï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰æ™‚ã‚†ã‚‹ã‚ï¼‰
    def _fits_len_trendaware(text: str) -> bool:
        if lang_code in ("ja","ko","zh"):
            limit = 36 if is_trend else 34
            return len(text or "") <= limit
        else:
            words = re.findall(r"\b\w+\b", text or "")
            limit = 14 if is_trend else 13
            return len(words) <= limit

    tries = 7 if is_trend else 5
    for attempt in range(1, tries + 1):
        # æœ€å¾Œã®2å›ã¯ã‚·ãƒ³ãƒ—ãƒ«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åˆ‡æ›¿
        use_simple = (attempt >= tries - 1)
        prompt_user = user_simple if use_simple else user_normal
        try:
            rsp = GPT.chat.completions.create(
                model="gpt-4o-mini",
                messages=[system, {"role":"user","content":prompt_user}],
                temperature=local_temp if not use_simple else 0.05,
                top_p=0.9,
                presence_penalty=0, frequency_penalty=0,
            )
            raw = (rsp.choices[0].message.content or "").strip()
        except Exception:
            raw = ""

        cand = _clean_strict(raw)
        ok_single = _is_single_sentence(cand)
        ok_len    = _fits_len_trendaware(cand)
        ok_contain= _contains_token_trendaware(word, cand, lang_code, is_trend)
        valid = bool(cand) and ok_single and ok_len and ok_contain
        if valid:
            return _ensure_period_for_sentence(cand, lang_code)

        if DEBUG_SCRIPT:
            try:
                dbg = TEMP / "example_debug.tsv"
                dbg.parent.mkdir(parents=True, exist_ok=True)
                with open(dbg, "a", encoding="utf-8") as f:
                    f.write("\t".join([
                        "word="+word,
                        "attempt="+str(attempt),
                        "cand="+cand.replace("\t"," "),
                        "single="+str(ok_single),
                        "len="+str(ok_len),
                        "contain="+str(ok_contain),
                        "\n"
                    ]))
            except Exception:
                pass

    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    if lang_code == "ja":
        return _ja_template_fallback(word)
    elif lang_code == "es":
        return _ensure_period_for_sentence(f"Practiquemos {word}", lang_code)
    elif lang_code == "pt":
        return _ensure_period_for_sentence(f"Vamos praticar {word}", lang_code)
    elif lang_code == "fr":
        return _ensure_period_for_sentence(f"Pratiquons {word}", lang_code)
    elif lang_code == "id":
        return _ensure_period_for_sentence(f"Ayo berlatih {word}", lang_code)
    elif lang_code == "ko":
        return _ensure_period_for_sentence(f"{word}ë¥¼ ì—°ìŠµí•´ ë´…ì‹œë‹¤", lang_code)
    else:
        return _ensure_period_for_sentence(f"Let's practice {word}", lang_code)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â˜… specå¯¾å¿œã®èªå½™ç”Ÿæˆ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _gen_vocab_list_from_spec(spec: dict, lang_code: str) -> list[str]:
    n   = int(spec.get("count", int(os.getenv("VOCAB_WORDS", "6"))))
    th  = spec.get("theme") or "general vocabulary"
    pos = spec.get("pos") or []
    rel = (spec.get("relation_mode") or "").strip().lower()
    diff = (spec.get("difficulty") or "").strip().upper()
    patt = (spec.get("pattern_hint") or "").strip()
    morph = spec.get("morphology") or []
    is_trend = bool(spec.get("trend"))

    theme_for_prompt = translate(th, lang_code) if lang_code != "en" else th

    lines = []
    lines.append(f"You are selecting {n} HIGH-FREQUENCY words for the topic: {theme_for_prompt}.")
    if pos: lines.append("Restrict part-of-speech to: " + ", ".join(pos) + ".")
    if rel == "synonym":
        lines.append("Prefer synonyms or near-synonyms around the central topic.")
    elif rel == "antonym":
        lines.append("Include at least one meaningful antonym pair if possible.")
    elif rel == "collocation":
        lines.append("Prefer common collocations used with the topic in everyday speech.")
    elif rel == "pattern":
        lines.append("Prefer short reusable patterns or set phrases.")
    elif rel == "contextual":
        lines.append("Focus on words useful in natural conversation about this topic: opinions, reasons, actions, feelings, plans, tickets, time, place.")
        lines.append("Avoid rare proper nouns; prefer reusable mid-frequency conversation words.")

    if patt:
        lines.append(f"Pattern focus hint: {patt}.")
    if morph:
        lines.append("If natural, include related morphological family: " + ", ".join(morph) + ".")
    if is_trend:
        lines.append("Do NOT include names of people, teams, brands, hashtags, usernames, or titles of works.")
        lines.append("Avoid multi-word proper nouns and words that require capitalization in general use.")
        lines.append("Prefer single tokens that can be used in everyday sentences about the topic.")

    if diff in ("A1","A2","B1"):
        lines.append(f"Target approximate CEFR level: {diff}. Keep words short and common for this level.")

    lines.append("Return ONLY one word or short hyphenated term per line, no numbering, no punctuation.")
    lines.append(f"All words must be written in {LANG_NAME.get(lang_code,'the target')} language.")
    prompt = "\n".join(lines)

    content = ""
    try:
        rsp = GPT.chat_completions.create(  # â† ä¸€éƒ¨ã®ç’°å¢ƒã§ chat.completions ã«ã‚¨ã‚¤ãƒªã‚¢ã‚¹
            model="gpt-4o-mini",
            messages=[{"role":"user","content":prompt}],
            temperature=LIST_TEMP, top_p=0.9,
            presence_penalty=0, frequency_penalty=0,
        )
        content = (rsp.choices[0].message.content or "")
    except Exception:
        try:
            rsp = GPT.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role":"user","content":prompt}],
                temperature=LIST_TEMP, top_p=0.9,
                presence_penalty=0, frequency_penalty=0,
            )
            content = (rsp.choices[0].message.content or "")
        except Exception:
            content = ""

    words = []
    for line in content.splitlines():
        w = (line or "").strip()
        if not w: continue
        w = re.sub(r"^\d+[\).]?\s*", "", w)
        w = re.sub(r"[ï¼Œã€ã€‚.!?ï¼ï¼Ÿ]+$", "", w)
        w = w.split()[0]
        if not w:
            continue

        if is_trend:
            if re.search(r"[#@/0-9]", w):
                continue
            if lang_code == "en" and len(w) >= 2 and w[0].isupper() and w[1:].islower():
                continue

        if w not in words:
            words.append(w)

    if len(words) >= n:
        return words[:n]
    fallback = ["check-in", "reservation", "checkout", "receipt", "elevator", "lobby", "upgrade"]
    return (words + [w for w in fallback if w not in words])[:n]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â˜… spec æ­£è¦åŒ–ï¼‹specå¯¾å¿œã®èªå½™ç”Ÿæˆ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _normalize_spec(picked, context_hint, audio_lang, words_env_count: int):
    if isinstance(picked, dict):
        theme = picked.get("theme") or "general vocabulary"
        ctx   = picked.get("context") or (context_hint or "")
        spec  = dict(picked)
        if "count" not in spec or not isinstance(spec["count"], int):
            spec["count"] = words_env_count
        return theme, ctx, spec

    if isinstance(picked, tuple) and len(picked) == 2:
        theme, ctx = picked[0], picked[1]
        spec = {
            "theme": theme,
            "context": ctx or (context_hint or ""),
            "count": words_env_count
        }
        return theme, ctx, spec

    theme = str(picked)
    ctx   = context_hint or ""
    spec  = {
        "theme": theme,
        "context": ctx,
        "count": words_env_count
    }
    return theme, ctx, spec

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â˜… ãƒ†ãƒ¼ãƒã®ã¿ã‹ã‚‰èªå½™ã‚’å¼•ãï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼äº’æ›ç”¨ï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _gen_vocab_list(theme: str, lang_code: str, n: int) -> list[str]:
    theme_for_prompt = translate(theme, lang_code) if lang_code != "en" else theme
    lines = [
        f"Select {n} high-frequency words in {LANG_NAME.get(lang_code,'the target')} for the topic: {theme_for_prompt}.",
        "Rules:",
        "- Output ONLY one common word per line (single token or short hyphenated term).",
        "- No numbering, no punctuation, no emojis, no hashtags, no URLs.",
        "- Avoid proper names, brands, and titles.",
        "- Prefer words that fit everyday conversation about the topic.",
    ]
    prompt = "\n".join(lines)
    try:
        rsp = GPT.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role":"user","content":prompt}],
            temperature=LIST_TEMP, top_p=0.9,
        )
        content = (rsp.choices[0].message.content or "")
    except Exception:
        content = ""

    out = []
    for line in content.splitlines():
        w = (line or "").strip()
        if not w: continue
        w = re.sub(r"^\d+[\).]?\s*", "", w)
        w = re.sub(r"[ï¼Œã€ã€‚.!?ï¼ï¼Ÿ]+$", "", w)
        w = w.split()[0]
        if not w: continue
        if re.search(r"[#@/0-9]", w):
            continue
        if w not in out:
            out.append(w)
    if len(out) >= n:
        return out[:n]
    fallback = ["hello","thanks","sorry","because","however","therefore"]
    return (out + [w for w in fallback if w not in out])[:n]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# æ—¥æœ¬èªTTSç”¨ãµã‚ŠãŒãªï¼ˆèªãƒ»æ–‡ï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_KANJI_ONLY = re.compile(r"^[ä¸€-é¾¥ã€…]+$")
_PARENS_JA  = re.compile(r"\s*[\(\ï¼ˆ][^)\ï¼‰]{1,40}[\)\ï¼‰]\s*")
_KANJI_CHAR = re.compile(r"[ä¸€-é¾¥ã€…]")

def _kana_reading(word: str) -> str:
    try:
        rsp = GPT.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{
                "role":"user",
                "content":(
                    "æ¬¡ã®æ—¥æœ¬èªå˜èªã®èª­ã¿ã‚’ã²ã‚‰ãŒãªã ã‘ã§1èªè¿”ã—ã¦ãã ã•ã„ã€‚"
                    "è¨˜å·ãƒ»æ‹¬å¼§ãƒ»èª¬æ˜ã¯ä¸è¦ã€‚\n"
                    f"å˜èª: {word}"
                )
            }],
            temperature=0.0, top_p=1.0,
        )
        yomi = (rsp.choices[0].message.content or "").strip()
        yomi = re.sub(r"[^ã-ã‚–ã‚ã‚ãƒ¼]+", "", yomi)
        return yomi[:20]
    except Exception:
        return ""

def _kanji_ratio(s: str) -> float:
    t = re.sub(r"[ \t\u3000]", "", s or "")
    if not t:
        return 0.0
    kan = len(_KANJI_CHAR.findall(t))
    return kan / max(1, len(t))

def _kana_reading_sentence(text: str) -> str:
    src = (text or "").strip()
    if not src:
        return ""
    try:
        rsp = GPT.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{
                "role":"user",
                "content":(
                    "æ¬¡ã®æ—¥æœ¬èªã®æ–‡ã‚’ã€ãã®ã¾ã¾ã®æ„å‘³ã§ **ã²ã‚‰ãŒãªã ã‘** ã«ç›´ã—ã¦è¿”ã—ã¦ãã ã•ã„ã€‚"
                    "å¥èª­ç‚¹ï¼ˆã€ã€‚ï¼ï¼Ÿï¼‰ã¯ãã®ã¾ã¾æ®‹ã—ã¦ã‹ã¾ã„ã¾ã›ã‚“ã€‚"
                    "ã‚«ã‚¿ã‚«ãƒŠãƒ»æ¼¢å­—ãƒ»è‹±å­—ãƒ»æ•°å­—ãƒ»è¨˜å·ã¯ä½¿ã‚ãªã„ã§ãã ã•ã„ã€‚\n\n"
                    f"æ–‡: {src}"
                )
            }],
            temperature=0.0, top_p=1.0,
        )
        out = (rsp.choices[0].message.content or "").strip()
        out = re.sub(r"[^ã-ã‚–ã‚ã‚ãƒ¼ã€ã€‚ï¼ï¼Ÿ\s]+", "", out)
        out = _normalize_spaces(out)
        if out and not re.search(r"[ã€‚.!?ï¼ï¼Ÿ]$", out):
            out += "ã€‚"
        return out
    except Exception:
        return ""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ‡¯ğŸ‡µğŸ‡°ğŸ‡· ãƒ­ãƒ¼ãƒå­—å­—å¹•ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆä»»æ„ä¾å­˜ï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    from pykakasi import kakasi
    _KK = kakasi()
    _KK.setMode('H', 'a')  # ã²ã‚‰ãŒãªâ†’ãƒ­ãƒ¼ãƒå­—
    _KK.setMode('K', 'a')  # ã‚«ã‚¿ã‚«ãƒŠâ†’ãƒ­ãƒ¼ãƒå­—
    _KK.setMode('J', 'a')  # æ¼¢å­—â†’ãƒ­ãƒ¼ãƒå­—
    _KK_CONV = _KK.getConverter()
except Exception:
    _KK_CONV = None

try:
    from hangul_romanize import Transliter
    from hangul_romanize.rule import academic
    _KO_TR = Transliter(academic)
except Exception:
    _KO_TR = None

def _to_romaji_ja(text: str) -> str:
    if not _KK_CONV:
        return ""
    return _KK_CONV.do(text or "")

def _to_roman_ko(text: str) -> str:
    if not _KO_TR:
        return ""
    return _KO_TR.translit(text or "")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ä¾‹æ–‡å–å¾—ï¼ˆåŒã˜3è¡Œãƒ–ãƒ­ãƒƒã‚¯ã®3è¡Œç›®ï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _example_for_index(valid_dialogue: list[tuple[str, str]], idx0: int) -> str:
    role_idx = idx0 % 3
    base = idx0 - role_idx
    ex_pos = base + 2
    if 0 <= ex_pos < len(valid_dialogue):
        return valid_dialogue[ex_pos][1]
    return ""

def _safeify_trend_title(theme: str, lang: str) -> str:
    risk_words = [
        "war","attack","sex","murder","killed","politics","election","vote","president",
        "trump","biden","putin","israel","gaza","palestine","covid","vaccine","virus",
        "drug","explosion","shooting","scandal","accident","terror","suicide","death",
        "earthquake","tsunami","hurricane","flood","protest","riot","bomb","hostage"
    ]
    text = theme.strip().lower()
    risky = any(word in text for word in risk_words)
    if not risky:
        return theme
    safe_titles = {
        "ja": "æœ€è¿‘ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’é¡Œæã«ã—ãŸèªå½™å­¦ç¿’",
        "en": "Vocabulary from recent news topics",
        "ko": "ìµœê·¼ ë‰´ìŠ¤ ì£¼ì œì—ì„œ ë°°ìš°ëŠ” ì–´íœ˜",
        "es": "Vocabulario de temas de actualidad",
        "fr": "Vocabulaire Ã  partir des sujets dâ€™actualitÃ©",
        "pt": "VocabulÃ¡rio de tÃ³picos recentes de notÃ­cias",
        "id": "Kosakata dari topik berita terkini",
    }
    return safe_titles.get(lang, safe_titles["en"])

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å˜èªå°‚ç”¨ãƒ»æ–‡è„ˆã¤ãç¿»è¨³ï¼ˆ1èªã ã‘è¿”ã™ï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def translate_word_context(word: str, target_lang: str, src_lang: str, theme: str, example: str, pos_hint: str | None = None) -> str:
    theme = (theme or "").strip()
    example = (example or "").strip()
    pos_line = f"Part of speech hint: {pos_hint}." if pos_hint else ""

    prompt_lines = [
        "You are a precise bilingual lexicon generator.",
        f"Source language code: {src_lang}",
        f"Target language code: {target_lang}",
        "Task: Translate the SINGLE WORD below into exactly ONE natural target-language word that matches the intended meaning in the given context.",
        "Rules:",
        "- Output ONLY one word, no punctuation, no quotes, no explanations.",
        "- The output must be written entirely in the target language.",
        f"- Return ONLY one word in {LANG_NAME.get(target_lang,'target language')} language.",
        "- Choose the sense that fits the context (theme and example sentence).",
        "- Avoid month-name or book-title senses unless clearly indicated by context.",
        pos_line,
        "",
        f"Word: {word}",
        f"Theme: {theme}" if theme else "Theme: (none)",
        f"Example sentence for context: {example}" if example else "Example sentence for context: (none)",
    ]
    try:
        rsp = GPT.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role":"user","content":"\n".join(prompt_lines)}],
            temperature=0.0, top_p=1.0
        )
        out = (rsp.choices[0].message.content or "").strip()
        out = re.sub(r"[ï¼Œã€ã€‚.!?ï¼ï¼Ÿ]+$", "", out).strip()
        out = out.split()[0] if out else ""
        return out or word
    except Exception:
        return word

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ãƒ¡ã‚¿ç”Ÿæˆï¼ˆã‚¿ã‚¤ãƒˆãƒ«è¨€èªã«çµ±ä¸€ï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def make_title(theme, title_lang: str, audio_lang_for_label: str | None = None):
    import re
    import random
    from pathlib import Path

    ONE_MIN_TAG = {
        "ja": " 1åˆ†å­¦ç¿’",
        "en": " 1 min vocabulary",
        "es": " 1 min vocabulario",
        "fr": " 1 min vocabulaire",
        "pt": " 1 min vocabulÃ¡rio",
        "id": " 1 menit kosakata",
        "ko": " 1ë¶„ ì–´íœ˜",
    }

    if title_lang not in LANG_NAME:
        title_lang = "en"
    max_len = int(os.getenv("TITLE_MAX_JA", "28")) if title_lang == "ja" else int(os.getenv("TITLE_MAX_OTHER", "55"))

    try:
        theme_local = theme if title_lang == "en" else translate(theme, title_lang)
    except Exception:
        theme_local = theme

    theme_local = _safeify_trend_title(theme_local, title_lang)

    prefix = ""
    if title_lang == "ja":
        label = JP_CONV_LABEL.get((audio_lang_for_label or "").strip(), "")
        if label:
            prefix = f"{label} "

    def _smart_trim(s: str, lim: int) -> str:
        if len(s) <= lim:
            return s
        s = s[:lim]
        return re.sub(r"[ \u3000ãƒ»ã€ã€‚!ï¼?ï¼Ÿ:ï¼š\-â€“â€”ï½œ|]+$", "", s)

    style = os.getenv("TITLE_STYLE", "random").lower()
    if   style == "classic": mode = 0
    elif style == "fixed":   mode = 1
    else:                    mode = random.choice([0, 1])

    def _fixed_1min() -> str:
        tag = ONE_MIN_TAG.get(title_lang, "~1 min vocabulary")
        if title_lang == "ja":
            t = f"{prefix}{sanitize_title(theme_local)}ï½œ{tag}"
        else:
            t = f"{sanitize_title(theme_local)} | {tag}"
        return _smart_trim(sanitize_title(t), max_len)

    def _classic() -> str:
        try:
            lang_name = LANG_NAME.get(title_lang, "English")
            style_hint = "Keep it simple, catchy, and useful for learners. No emojis, no quotes."
            if title_lang == "ja":
                style_hint += " Start the title exactly with the given prefix. Keep consistent phrasing."

            prompt = (
                f"Generate ONE short YouTube title in {lang_name} for a vocabulary-learning short.\n"
                f"Theme: {theme_local}\n"
                f"Max characters: {max_len}\n"
                f"{style_hint}\n"
            )
            if prefix:
                prompt += f"Prefix (must appear at the very beginning): '{prefix}'\n"

            rsp = GPT.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0, top_p=1.0,
            )
            t = (rsp.choices[0].message.content or "").strip()
            t = sanitize_title(t)
            if title_lang == "ja" and prefix and not t.startswith(prefix):
                t = prefix + t
            t = _smart_trim(t, max_len)
            if len(t) < 4:
                raise ValueError("too short")
            return t
        except Exception:
            if title_lang == "ja":
                base = f"{theme_local} ã§ä½¿ãˆã‚‹ä¸€è¨€"
                t = (prefix + base) if prefix and not base.startswith(prefix) else base
                return _smart_trim(sanitize_title(t), max_len)
            if title_lang == "en":
                t = f"{theme_local.capitalize()} vocab in one minute"
            else:
                t = f"{theme_local} vocabulary"
            return _smart_trim(sanitize_title(t), max_len)

    return _fixed_1min() if mode == 1 else _classic()

def make_desc(theme, title_lang: str):
    if title_lang not in LANG_NAME:
        title_lang = "en"
    try:
        theme_local = theme if title_lang == "en" else translate(theme, title_lang)
    except Exception:
        theme_local = theme
    msg = {
        "ja": f"{theme_local} ã«å¿…é ˆã®èªå½™ã‚’çŸ­æ™‚é–“ã§ãƒã‚§ãƒƒã‚¯ã€‚å£°ã«å‡ºã—ã¦ä¸€ç·’ã«ç·´ç¿’ã—ã‚ˆã†ï¼ #vocab #learning",
        "en": f"Quick practice for {theme_local} vocabulary. Repeat after the audio! #vocab #learning",
        "pt": f"Pratique rÃ¡pido o vocabulÃ¡rio de {theme_local}. Repita em voz alta! #vocab #aprendizado",
        "es": f"PrÃ¡ctica rÃ¡pida de vocabulario de {theme_local}. Â¡Repite en voz alta! #vocab #aprendizaje",
        "ko": f"{theme_local} ì–´íœ˜ë¥¼ ë¹ ë¥´ê²Œ ì—°ìŠµí•˜ì„¸ìš”. ì†Œë¦¬ ë‚´ì–´ ë”°ë¼ ë§í•´ìš”! #vocab #learning",
        "id": f"Latihan cepat kosakata {theme_local}. Ucapkan keras-keras! #vocab #belajar",
    }
    return msg.get(title_lang, msg["en"])

def _make_trend_context(theme: str, lang_code: str) -> str:
    theme = (theme or "").strip()
    if not theme:
        return ""
    if lang_code == "ja":
        return (
            f"ä»Šã“ã®è©±é¡Œï¼ˆ{theme}ï¼‰ã«ã¤ã„ã¦å‹é”ã¨é›‘è«‡ã€‚äºˆå®šã‚„æ„Ÿæƒ³ã€å ´æ‰€ã‚„æ™‚é–“ã€ãƒã‚±ãƒƒãƒˆã€æ··é›‘ã€"
            "ãƒ‹ãƒ¥ãƒ¼ã‚¹ã§è¦‹ãŸå†…å®¹ãªã©ã‚’è‡ªç„¶ã«è©±ã™ã€‚å°‚é–€ç”¨èªã¯é¿ã‘ã¦ã€æ—¥å¸¸ä¼šè©±ã§ã‚ˆãä½¿ã†èªã‚’å„ªå…ˆã€‚"
        )
    elif lang_code == "en":
        return (
            f"Casual small talk about '{theme}': plans, opinions, tickets, time, place, "
            "what you saw on the news. Prefer everyday words and useful collocations."
        )
    elif lang_code == "es":
        return (
            f"ConversaciÃ³n informal sobre '{theme}': planes, opiniones, entradas, horarios y lugar. "
            "Usa palabras cotidianas y colocaciones frecuentes."
        )
    elif lang_code == "fr":
        return (
            f"Petite discussion sur '{theme}': projets, avis, billets, horaires, lieu. "
            "PrivilÃ©gier le vocabulaire courant et les collocations utiles."
        )
    elif lang_code == "pt":
        return (
            f"Bate-papo sobre '{theme}': planos, opiniÃ£o, ingressos, horÃ¡rio e local. "
            "Prefira palavras do dia a dia e colocaÃ§Ãµes frequentes."
        )
    elif lang_code == "id":
        return (
            f"Obrolan santai tentang '{theme}': rencana, pendapat, tiket, waktu, tempat. "
            "Gunakan kosakata sehari-hari dan kolokasi umum."
        )
    elif lang_code == "ko":
        return (
            f"'{theme}'ì— ëŒ€í•´ ê°€ë³ê²Œ ëŒ€í™”: ê³„íš, ì˜ê²¬, í‹°ì¼“, ì‹œê°„, ì¥ì†Œ. "
            "ì¼ìƒì ìœ¼ë¡œ ìì£¼ ì“°ëŠ” í‘œí˜„ê³¼ ì—°ì–´ã‚’ ìš°ì„ í•œë‹¤."
        )
    return f"Casual talk about '{theme}' with everyday words (plans, opinions, tickets, time, place)."

def make_tags(theme, audio_lang, subs, title_lang):
    LOCALIZED_TAGS = {
        "ja": ["èªå½™", "å˜èªå­¦ç¿’", "ãƒªã‚¹ãƒ‹ãƒ³ã‚°ç·´ç¿’", "ã‚¹ãƒ”ãƒ¼ã‚­ãƒ³ã‚°ç·´ç¿’", "å­—å¹•"],
        "en": ["vocabulary", "language learning", "speaking practice", "listening practice", "subtitles"],
        "es": ["vocabulario", "aprendizaje de idiomas", "prÃ¡ctica oral", "prÃ¡ctica auditiva", "subtÃ­tulos"],
        "fr": ["vocabulaire", "apprentissage des langues", "pratique orale", "Ã©coute", "sous-titres"],
        "pt": ["vocabulÃ¡rio", "aprendizado de idiomas", "prÃ¡tica de fala", "prÃ¡tica auditiva", "legendas"],
        "id": ["kosakata", "belajar bahasa", "latihan berbicara", "latihan mendengarkan", "subtitle"],
        "ko": ["ì–´íœ˜", "ì–¸ì–´ å­¦ç¿’", "ë§í•˜ê¸° ì—°ìŠµ", "ë“£ê¸° ì—°ìŠµ", "ìë§‰"],
    }

    base_tags = LOCALIZED_TAGS.get(title_lang, LOCALIZED_TAGS["en"]).copy()

    try:
        theme_local = theme if title_lang == "en" else translate(theme, title_lang)
    except Exception:
        theme_local = theme
    base_tags.insert(0, theme_local)

    # ã‚¿ã‚°ã¯â€œå…ƒã®å­—å¹•è¨€èª(subs)â€ã®ã¿ï¼ˆja-Latn/ko-Latn ã¯å«ã‚ãªã„ï¼‰
    for code in subs:
        if code in LANG_NAME:
            if title_lang == "ja":
                base_tags.append(f"{LANG_NAME[code]} å­—å¹•")
            else:
                base_tags.append(f"{LANG_NAME[code]} subtitles")

    seen, out = set(), []
    for t in base_tags:
        if t not in seen:
            seen.add(t); out.append(t)
    return out[:15]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å˜ç´”çµåˆï¼ˆWAVä¸­é–“ãƒ»è¡Œé ­ç„¡éŸ³ãƒ»æœ€çŸ­å°ºãƒ»è¡Œé–“ã‚®ãƒ£ãƒƒãƒ—ï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _concat_with_gaps(audio_paths, gap_ms=120, pre_ms=120, min_ms=1000):
    combined = AudioSegment.silent(duration=0)
    durs = []
    for idx, p in enumerate(audio_paths):
        seg = AudioSegment.from_file(p)
        seg = AudioSegment.silent(duration=pre_ms) + seg
        if len(seg) < min_ms:
            seg += AudioSegment.silent(duration=min_ms - len(seg))
        seg_ms = len(seg)
        extra = gap_ms if idx < len(audio_paths) - 1 else 0
        combined += seg
        if extra:
            combined += AudioSegment.silent(duration=extra)
        durs.append((seg_ms + extra) / 1000.0)
    (TEMP / "full_raw.wav").unlink(missing_ok=True)
    combined.export(TEMP / "full_raw.wav", format="wav")
    return durs

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1ã‚³ãƒ³ãƒœå‡¦ç†
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run_one(topic, turns, audio_lang, subs, title_lang, yt_privacy, account, do_upload, chunk_size, context_hint="", spec=None):
    reset_temp()

    # â–¼â–¼â–¼ å­—å¹•ãƒ©ãƒ³ã‚¿ã‚¤ãƒ æ§‹æˆï¼šja/ko ã¯ 3æ®µå›ºå®šï¼ˆåŸæ–‡ / ãƒ­ãƒ¼ãƒå­— / ç¬¬2å­—å¹•ï¼‰ â–¼â–¼â–¼
    def _pick_secondary_sub(primary: str, subs_list: list[str]) -> str | None:
        """subs ã‹ã‚‰ primary ã¨ -Latn/ja-kana ã‚’é™¤ã„ãŸæœ€åˆã®è¨€èªã‚³ãƒ¼ãƒ‰ã‚’è¿”ã™"""
        for c in subs_list:
            if c == primary:
                continue
            if c.endswith("-Latn") or c == "ja-kana":
                continue
            return c
        return None

    if audio_lang in ("ja", "ko"):
        runtime_subs: list[str] = []
        # 1åˆ—ç›®: åŸæ–‡
        runtime_subs.append(audio_lang)
        # 2åˆ—ç›®: ãƒ­ãƒ¼ãƒå­—
        runtime_subs.append("ja-Latn" if audio_lang == "ja" else "ko-Latn")
        # 3åˆ—ç›®: ç¬¬2å­—å¹•ï¼ˆcombos.yaml ã® subs ã‹ã‚‰å–å¾—ã€‚ç„¡ã‘ã‚Œã° Fallbackï¼‰
        _sec = _pick_secondary_sub(audio_lang, subs)
        if _sec:
            runtime_subs.append(_sec)
        else:
            fb = os.getenv("FALLBACK_SECOND_SUB", "en")
            if fb != audio_lang:
                runtime_subs.append(fb)
    else:
        # ãã‚Œä»¥å¤–ã®éŸ³å£°è¨€èªã¯å¾“æ¥ã©ãŠã‚Š
        runtime_subs = list(subs)
        
        def _insert_after(lst, key, newcode):
            if key in lst and newcode not in lst:
                i = lst.index(key)
                lst.insert(i + 1, newcode)
        
        # ãµã‚ŠãŒãªè¿½åŠ ã®æ¡ä»¶ã‚’å³å¯†åŒ–ï¼š
        # rows=2ï¼ˆã¤ã¾ã‚Š subs ãŒ2è¨€èªï¼‰ãªã‚‰ ja-Latn ã¯è¿½åŠ ã—ãªã„
        if os.getenv("SUB_ROMAJI_JA", "0") == "1" and len(runtime_subs) < 2:
            _insert_after(runtime_subs, "ja", "ja-Latn")

    logging.info(f"[SUBS] runtime_subs={runtime_subs} (audio={audio_lang}, subs={subs})")
    # â–²â–²â–² ä»¥é™ã€å­—å¹•å‡¦ç†ãƒ»è¡Œæ•°ã¯ runtime_subs ã‚’ä½¿ç”¨ã€‚ãƒ¡ã‚¿ï¼ˆtitle/tagsç­‰ï¼‰ã¯ subs ã‚’ä½¿ç”¨ã€‚â–²â–²â–²
    
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ èªå½™ãƒªã‚¹ãƒˆç”Ÿæˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    raw = (topic or "").replace("\r", "\n").strip()
    is_word_list = bool(re.search(r"[,;\n]", raw)) and len([w for w in re.split(r"[\n,;]+", raw) if w.strip()]) >= 2

    words_count = int(os.getenv("VOCAB_WORDS", "6"))
    if is_word_list:
        vocab_words = [w.strip() for w in re.split(r"[\n,;]+", raw) if w.strip()]
        theme = "custom list"
        local_context = ""
    else:
        if isinstance(spec, dict):
            theme = spec.get("theme") or topic
            local_context = (spec.get("context") or context_hint or "")
            vocab_words = _gen_vocab_list_from_spec(spec, audio_lang)
            try:
                (TEMP / "spec.json").write_text(json.dumps(spec, ensure_ascii=False, indent=2), encoding="utf-8")
            except Exception:
                pass
        else:
            theme = topic
            vocab_words = _gen_vocab_list(theme, audio_lang, words_count)
            local_context = context_hint or ""

    # 3è¡Œãƒ–ãƒ­ãƒƒã‚¯: å˜èª â†’ å˜èª â†’ ä¾‹æ–‡
    dialogue = []
    for w in vocab_words:
        diff = (spec.get("difficulty") if isinstance(spec, dict) else None)
        ex = _gen_example_sentence(w, audio_lang, local_context, difficulty=diff)
        dialogue.extend([("N", w), ("N", w), ("N", ex)])

    valid_dialogue = [(spk, line) for (spk, line) in dialogue if line.strip()]
    audio_parts, sub_rows = [], [[] for _ in runtime_subs]
    plain_lines, tts_lines = [line for (_, line) in valid_dialogue], []

    for i, (spk, line) in enumerate(valid_dialogue, 1):
        role_idx = (i - 1) % 3  # 0/1=å˜èªè¡Œ, 2=ä¾‹æ–‡è¡Œ

        # -------- TTS ãƒ†ã‚­ã‚¹ãƒˆæ•´å½¢ --------
        tts_line = line
        if audio_lang == "ja":
            if role_idx == 2:
                base_ex = _PARENS_JA.sub(" ", tts_line).strip()
                base_ex = _ensure_period_for_sentence(base_ex, audio_lang)

                do_kana = False
                if JA_EX_READING == "on":
                    do_kana = True
                elif JA_EX_READING == "auto":
                    if 2 <= len(base_ex) <= JA_EX_READING_MAX_LEN and _kanji_ratio(base_ex) >= JA_EX_READING_KANJI_RATIO:
                        do_kana = True

                if do_kana:
                    yomi_ex = _kana_reading_sentence(base_ex)
                    tts_line = yomi_ex or base_ex
                else:
                    tts_line = base_ex
            else:
                if _KANJI_ONLY.fullmatch(line):
                    yomi = _kana_reading(line)
                    if yomi:
                        tts_line = yomi
                base = re.sub(r"[ã€‚ï¼ï¼Ÿ!?]+$", "", tts_line).strip()
                tts_line = base + "ã€‚" if len(base) >= 2 else base
        else:
            if role_idx == 2:
                tts_line = _ensure_period_for_sentence(tts_line, audio_lang)

        out_audio = TEMP / f"{i:02d}.wav"
        style_for_tts = "serious" if audio_lang == "ja" else "neutral"
        speak(audio_lang, spk, tts_line, out_audio, style=style_for_tts)
        audio_parts.append(out_audio)
        tts_lines.append(tts_line)

        # -------- å­—å¹•ç”Ÿæˆï¼ˆruntime_subsï¼‰ --------
        for r, lang in enumerate(runtime_subs):
            if lang == audio_lang:
                # åŸæ–‡
                sub_rows[r].append(_clean_sub_line(line, lang))

            elif lang == "ja-kana":
                # ãµã‚ŠãŒãªè¡Œï¼šå˜èªè¡Œã¯å˜èªã‹ãªã€ä¾‹æ–‡è¡Œã¯æ–‡ã‹ãª
                base_ja = _clean_sub_line(line, "ja")
                if role_idx in (0, 1):
                    if _KANJI_ONLY.fullmatch(base_ja):
                        kana = _kana_reading(base_ja) or base_ja
                    else:
                        # å˜èªã§ã‚‚æ¼¢å­—æ··ã˜ã‚Šç­‰ã¯å®‰å…¨ã«æ–‡ã‹ãªåŒ–
                        kana = _kana_reading_sentence(base_ja) or base_ja
                else:
                    kana = _kana_reading_sentence(base_ja) or base_ja
                sub_rows[r].append(kana)

            elif lang == "ja-Latn":
                base_ja = _clean_sub_line(line, "ja")
                roma = _to_romaji_ja(base_ja) or base_ja
                sub_rows[r].append(roma)

            elif lang == "ko-Latn":
                base_ko = _clean_sub_line(line, "ko")
                roma = _to_roman_ko(base_ko) or base_ko
                sub_rows[r].append(roma)

            else:
                # ç¿»è¨³ç³»
                try:
                    if role_idx in (0, 1):
                        example_ctx = _example_for_index(valid_dialogue, i-1)
                        pos_hint = None
                        if isinstance(spec, dict) and spec.get("pos"):
                            pos_hint = ",".join(spec["pos"])
                        elif audio_lang == "ja":
                            _k = _guess_ja_pos(line)
                            pos_map = {"verb":"verb", "iadj":"adjective", "naadj":"adjective", "noun":"noun"}
                            pos_hint = pos_map.get(_k, None)
                        trans = translate_word_context(
                            word=line, target_lang=lang, src_lang=audio_lang,
                            theme=theme, example=example_ctx, pos_hint=pos_hint
                        )
                    else:
                        trans = translate_sentence_strict(line, src_lang=audio_lang, target_lang=lang)
                except Exception:
                    trans = line
                sub_rows[r].append(_clean_sub_line(trans, lang))

    # å˜ç´”çµåˆ â†’ æ•´éŸ³ â†’ mp3
    gap_ms = GAP_MS_JA if audio_lang == "ja" else GAP_MS
    pre_ms = PRE_SIL_MS_JA if audio_lang == "ja" else PRE_SIL_MS
    min_ms = MIN_UTTER_MS_JA if audio_lang == "ja" else MIN_UTTER_MS

    new_durs = _concat_with_gaps(audio_parts, gap_ms=gap_ms, pre_ms=pre_ms, min_ms=min_ms)
    enhance(TEMP/"full_raw.wav", TEMP/"full.wav")
    AudioSegment.from_file(TEMP/"full.wav").export(TEMP/"full.mp3", format="mp3")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ èƒŒæ™¯ç”»åƒï¼ˆå¿…ãšä½œã‚‹ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    bg_png = TEMP / "bg.png"
    try:
        theme_en = translate(theme, "en")
    except Exception:
        theme_en = theme
    first_word = valid_dialogue[0][1] if valid_dialogue else theme

    def _is_ascii(s: str) -> bool:
        try:
            s.encode("ascii"); return True
        except Exception:
            return False

    if not _is_ascii(first_word or ""):
        query_for_bg = theme_en or "language learning"
    else:
        query_for_bg = first_word or theme_en or "learning"

    try:
        fetch_bg(query_for_bg, bg_png)
        if not bg_png.exists():
            raise FileNotFoundError("bg.png not created by fetch_bg")
    except Exception:
        try:
            from PIL import Image
            Image.new("RGB", (1920, 1080), (240, 240, 240)).save(bg_png)
        except Exception as e:
            raise RuntimeError(f"Failed to prepare bg.png fallback: {e}")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ lines.jsonï¼ˆèƒŒæ™¯ç”Ÿæˆå¾Œã«ä½œæˆï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    lines_data = []
    for i, ((spk, txt), dur) in enumerate(zip(valid_dialogue, new_durs)):
        row = [spk]
        for r in range(len(runtime_subs)):
            row.append(sub_rows[r][i])
        row.append(dur)
        lines_data.append(row)
    (TEMP/"lines.json").write_text(json.dumps(lines_data, ensure_ascii=False, indent=2), encoding="utf-8")

    # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
    try:
        (TEMP / "script_raw.txt").write_text("\n".join(plain_lines), encoding="utf-8")
        (TEMP / "script_tts.txt").write_text("\n".join(tts_lines), encoding="utf-8")
        with open(TEMP / "subs_table.tsv", "w", encoding="utf-8") as f:
            header = ["idx", "text"] + [f"sub:{code}" for code in runtime_subs]
            f.write("\t".join(header) + "\n")
            for idx in range(len(valid_dialogue)):
                row = [str(idx+1), _clean_sub_line(valid_dialogue[idx][1], audio_lang)]
                for r in range(len(runtime_subs)):
                    row.append(sub_rows[r][idx])
                f.write("\t".join(row) + "\n")
        with open(TEMP / "durations.txt", "w", encoding="utf-8") as f:
            total = 0.0
            for i, d in enumerate(new_durs, 1):
                total += d
                f.write(f"{i:02d}\t{d:.3f}s\n")
            f.write(f"TOTAL\t{total:.3f}s\n")
    except Exception as e:
        logging.warning(f"[DEBUG_SCRIPT] write failed: {e}")

    if args.lines_only:
        return

    # ã‚µãƒ ãƒï¼ˆâ€» ã‚µãƒ ãƒ/ãƒ¡ã‚¿ã¯â€œå…ƒã® subsâ€åŸºæº–ã§æ±ºã‚ã‚‹ï¼‰
    thumb = TEMP / "thumbnail.jpg"
    thumb_lang = subs[1] if len(subs) > 1 else audio_lang
    make_thumbnail(theme, thumb_lang, thumb)

    # å‹•ç”»ç”Ÿæˆ
    stamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    final_mp4 = OUTPUT / f"{audio_lang}-{'_'.join(runtime_subs)}_{stamp}.mp4"
    final_mp4.parent.mkdir(parents=True, exist_ok=True)

    needed = {
        "lines.json": TEMP/"lines.json",
        "full.mp3":   TEMP/"full.mp3",
        "bg.png":     bg_png,
    }
    missing = [k for k,p in needed.items() if not p.exists()]
    if missing:
        raise RuntimeError(f"âŒ å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {', '.join(missing)}")

    cmd = [
        "python", str(BASE/"chunk_builder.py"),
        str(TEMP/"lines.json"), str(TEMP/"full.mp3"), str(bg_png),
        "--chunk", str(chunk_size),
        "--rows", str(len(runtime_subs)),
        "--out", str(final_mp4),
    ]
    logging.info("ğŸ”¹ chunk_builder cmd: %s", " ".join(cmd))
    subprocess.run(cmd, check=True)

    if not do_upload:
        return

    # ãƒ¡ã‚¿ç”Ÿæˆï¼†ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆtags/titleã¯ ja-Latn/ko-Latn ã‚’å«ã‚ãªã„ãŸã‚ subs ã‚’æ¸¡ã™ï¼‰
    title = make_title(theme, thumb_lang, audio_lang_for_label=audio_lang)
    desc  = make_desc(theme, thumb_lang)
    tags  = make_tags(theme, audio_lang, subs, thumb_lang)

    def _is_limit_error(err: Exception) -> bool:
        s = str(err)
        return (
            "uploadLimitExceeded" in s or
            "quotaExceeded" in s or
            "The user has exceeded the number of videos they may upload" in s
        )

    def _is_token_error(err: Exception) -> bool:
        s = str(err).lower()
        return (
            ("token" in s and "expired" in s) or
            ("invalid_grant" in s) or
            ("unauthorized_client" in s) or
            ("invalid_credentials" in s) or
            ("401" in s)
        )

    def _try_upload_with_fallbacks() -> bool:
        fb = os.getenv("UPLOAD_FALLBACKS", "").strip()
        fallbacks = [x.strip() for x in fb.split(",") if x.strip()]
        tried = []

        for acc in [account] + [a for a in fallbacks if a != account]:
            tried.append(acc)
            try:
                upload(
                    video_path=final_mp4, title=title, desc=desc, tags=tags,
                    privacy=yt_privacy, account=acc, thumbnail=thumb, default_lang=audio_lang
                )
                logging.info(f"[UPLOAD] âœ… success on account='{acc}'")
                return True
            except Exception as e:
                if _is_limit_error(e):
                    logging.warning(f"[UPLOAD] âš ï¸ limit reached on account='{acc}' â†’ trying next fallback.")
                    continue
                if _is_token_error(e):
                    logging.error(f"[UPLOAD] âŒ TOKEN ERROR on account='{acc}' â€” expired/revoked/unauthorized.")
                    try:
                        (TEMP / "TOKEN_EXPIRED.txt").write_text(
                            f"Token expired or revoked for account='{acc}'.\nDetail:\n{e}",
                            encoding="utf-8",
                        )
                    except Exception:
                        pass
                    raise SystemExit(f"[ABORT] Token expired/revoked for account='{acc}'. Please reauthorize.")
                logging.exception(f"[UPLOAD] unexpected error on account='{acc}'")
                raise

        msg = f"Upload skipped due to per-account limits. tried={tried}"
        logging.warning("[UPLOAD] " + msg)
        try:
            (TEMP / "UPLOAD_SKIPPED.txt").write_text(msg, encoding="utf-8")
        except Exception:
            pass
        return False

    _try_upload_with_fallbacks()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run_all(topic, turns, privacy, do_upload, chunk_size):
    """
    è¦ªãƒ—ãƒ­ã‚»ã‚¹: å„ account ã”ã¨ã«å­ãƒ—ãƒ­ã‚»ã‚¹ã‚’èµ·å‹•ï¼ˆISOLATED_RUN=1 ã‚’ä»˜ä¸ï¼‰
    å­ãƒ—ãƒ­ã‚»ã‚¹(ISOLATED_RUN=1): ãã®ã¾ã¾ run_one ã‚’å®Ÿè¡Œï¼ˆå†å¸°èµ·å‹•ã—ãªã„ï¼‰
    """
    isolated = os.getenv("ISOLATED_RUN", "0") == "1"

    if isolated:
        for combo in COMBOS:
            audio_lang  = combo["audio"]
            subs        = combo["subs"]
            account     = combo.get("account","default")
            title_lang  = _infer_title_lang(audio_lang, subs, combo)

            if TARGET_ONLY and account != TARGET_ONLY:
                continue

            picked_topic = topic
            context_hint = ""
            spec_for_run = None
            words_env_count = int(os.getenv("VOCAB_WORDS", "6"))

            # â˜… AUTO_TRENDï¼ˆé€šå¸¸å›ãªã‚‰é€šã‚‰ãªã„ï¼‰
            if topic.strip().upper() == "AUTO_TREND":
                try:
                    from topic_picker import build_trend_spec
                    TREND_DIR = TEMP / "trends"
                    trend_lang = subs[1] if len(subs) > 1 else audio_lang

                    candidates = get_trend_candidates(
                        audio_lang=trend_lang,
                        cache_dir=TREND_DIR,
                        limit=30,
                        cache_ttl_minutes=60,
                    )
                    if not candidates:
                        raise ValueError("no candidates")

                    today = int(dt.datetime.utcnow().strftime("%Y%m%d"))
                    idx = (hash((trend_lang, today)) % len(candidates))
                    picked_topic = candidates[idx]

                    spec_for_run = build_trend_spec(
                        theme=picked_topic,
                        audio_lang=audio_lang,
                        count=int(os.getenv("VOCAB_WORDS", "6")),
                    )
                    context_hint = _make_trend_context(picked_topic, audio_lang)

                    logging.info(
                        f"[TREND] trend_lang={trend_lang} â†’ picked='{picked_topic}' "
                        f"(#{idx+1}/{len(candidates)})"
                    )

                    try:
                        TREND_DIR.mkdir(parents=True, exist_ok=True)
                        with open(TREND_DIR / f"picked_{trend_lang}.txt", "w", encoding="utf-8") as f:
                            f.write("picked: " + picked_topic + "\n\n")
                            f.write("\n".join(candidates[:20]))
                    except Exception:
                        pass

                except Exception as e:
                    logging.warning(f"[TREND] failed ({e}) â†’ fallback to AUTO.")
                    topic = "AUTO"

            if topic.strip().lower() == "auto":
                try:
                    picked_raw = pick_by_content_type("vocab", audio_lang, return_context=True)
                    picked_topic, context_hint, spec_for_run = _normalize_spec(
                        picked_raw, context_hint, audio_lang, words_env_count
                    )
                except TypeError:
                    picked_raw = pick_by_content_type("vocab", audio_lang)
                    picked_topic, context_hint, spec_for_run = _normalize_spec(
                        picked_raw, context_hint, audio_lang, words_env_count
                    )

            logging.info(f"[ISOLATED] {audio_lang} | subs={subs} | account={account} | theme={picked_topic}")
            run_one(
                picked_topic, turns, audio_lang, subs, title_lang,
                privacy, account, do_upload, chunk_size,
                context_hint=context_hint, spec=spec_for_run
            )
        return

    for combo in COMBOS:
        account = combo.get("account", "default")
        if TARGET_ONLY and account != TARGET_ONLY:
            continue

        cmd = [
            sys.executable, str(BASE / "main.py"),
            topic,
            "--privacy", privacy,
            "--chunk", str(chunk_size),
            "--account", account,
        ]
        if not do_upload:
            cmd.append("--no-upload")

        env = os.environ.copy()
        env["ISOLATED_RUN"] = "1"

        logging.info(f"â–¶ Spawning isolated run for account={account}: {' '.join(cmd)}")
        subprocess.run(cmd, check=False, env=env)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
    ap = argparse.ArgumentParser()
    ap.add_argument("topic", help="èªå½™ãƒ†ãƒ¼ãƒã€‚AUTOï¼è‡ªå‹•ã€AUTO_TRENDï¼æœ€æ–°ãƒˆãƒ¬ãƒ³ãƒ‰ã‹ã‚‰å–å¾—ã€‚ã‚«ãƒ³ãƒ/æ”¹è¡ŒåŒºåˆ‡ã‚Šãªã‚‰å˜èªãƒªã‚¹ãƒˆã€‚")
    ap.add_argument("--turns", type=int, default=8)
    ap.add_argument("--privacy", default="unlisted", choices=["public","unlisted","private"])
    ap.add_argument("--lines-only", action="store_true")
    ap.add_argument("--no-upload", action="store_true")
    ap.add_argument("--chunk", type=int, default=9999, help="Shortsã¯åˆ†å‰²ã›ãš1æœ¬æ¨å¥¨")
    ap.add_argument("--account", type=str, default="", help="ã“ã® account ã®ã¿å®Ÿè¡Œï¼ˆcombos.yaml ã® account å€¤ã«ä¸€è‡´ï¼‰")
    args = ap.parse_args()

    target_cli = (args.account or "").strip()
    target_env = os.getenv("TARGET_ACCOUNT", "").strip()
    TARGET_ONLY = target_cli or target_env

    if TARGET_ONLY:
        selected = [c for c in COMBOS if c.get("account", "default") == TARGET_ONLY]
        if not selected:
            logging.error(f"[ABORT] No combos matched account='{TARGET_ONLY}'. Check combos.yaml.")
            raise SystemExit(2)
        COMBOS[:] = selected
        logging.info(f"[ACCOUNT FILTER] Running only for account='{TARGET_ONLY}' ({len(COMBOS)} combo(s)).")

    topic = resolve_topic(args.topic)
    run_all(topic, args.turns, args.privacy, not args.no_upload, args.chunk)